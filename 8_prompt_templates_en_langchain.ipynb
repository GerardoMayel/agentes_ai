{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates"
      ],
      "metadata": {
        "id": "PhE6VufOCybe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJTayxkkCgN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900b78ed-d636-4392-f7c1-2d056bfdccbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain"
      ],
      "metadata": {
        "id": "V7aVD6feCxA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa2a52e-9b1a-4e25-9dc3-b189f9c0f19d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "wKWT6OysC0Tj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template('Dime un chiste {topic}')\n",
        "\n",
        "print(prompt_template.invoke({'topic':'gatos'}))"
      ],
      "metadata": {
        "id": "P6XvoRU6C1vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37b6ba9-8bc1-480f-831a-81edb51526db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Dime un chiste gatos'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Eres un comediante mexicano'),\n",
        "    ('user', 'Dime un chiste {topic}')\n",
        "])\n",
        "print(prompt_template.invoke({'topic':'gatos'}))"
      ],
      "metadata": {
        "id": "cS0Ux3OBC25B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0f80f4-7a36-4401-f8e9-2d25843f5bde"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Eres un comediante mexicano', additional_kwargs={}, response_metadata={}), HumanMessage(content='Dime un chiste gatos', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0gkZ8gO5cVIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Eres un asistente útil'),\n",
        "    MessagesPlaceholder('msgs')\n",
        "\n",
        "])\n",
        "\n",
        "print(prompt_template.invoke({'msgs': [HumanMessage(content='hola'),HumanMessage(content='adios')]}))"
      ],
      "metadata": {
        "id": "9mqWzeUgC4X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6f0b00-443f-4628-ea9e-edf60cfa5ec2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Eres un asistente útil', additional_kwargs={}, response_metadata={}), HumanMessage(content='hola', additional_kwargs={}, response_metadata={}), HumanMessage(content='adios', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Un prompt template es una estructura que toma variables de entrada y las convierte en una instrucción clara para un modelo de lenguaje. Estas variables se pasan en un diccionario donde cada clave representa un valor que será utilizado para completar el template. Hay diferentes tipos de templates:\n",
        "\n",
        "Conceptos útiles:\n",
        "\n",
        "String Prompt Template: Es el tipo más sencillo, donde se toma una entrada y se convierte en una cadena de texto. Se usa para crear prompts básicos.\n",
        "Ejemplo: Un template podría ser \"Dime un chiste sobre {topic}\". Luego, se pasa un diccionario con {\"topic\": \"gatos\"}, y la salida sería \"Dime un chiste sobre gatos\".\n",
        "Chat Prompt Template: Se utiliza para crear una conversación con diferentes roles, como el sistema, el usuario, o un asistente. Permite generar listas de mensajes estructuradas.\n",
        "Ejemplo: El sistema podría ser \"Eres un asistente útil\", y el usuario podría decir \"Dime un chiste sobre perros\". El template organizaría esta conversación.\n",
        "Message Placeholder: Un lugar reservado en el template donde se puede insertar una lista de mensajes. Esto permite añadir múltiples mensajes en un punto específico del template, ideal para diálogos flexibles.\n",
        "Ejemplo: En un template de chat, podrías tener un placeholder donde se inserten varios mensajes del usuario como \"Hola\" y \"¿Cómo estás?\".\n",
        "Invoke: Método que permite pasar el diccionario de entrada al template y generar la salida final. Esto reemplaza las variables del template con los valores proporcionados.\n",
        "Ejemplo: Usando el template \"Dime un chiste sobre {topic}\", al invocar con {\"topic\": \"perros\"}, la salida sería \"Dime un chiste sobre perros\".\n",
        "Ejemplos:\n",
        "\n",
        "String Prompt Template template = \"Dime un chiste sobre {topic}\" input_data = {\"topic\": \"gatos\"} resultado = template.format(**input_data)  # Salida: \"Dime un chiste sobre gatos\"\n",
        "Chat Prompt Template: system_message = \"Eres un asistente útil\" user_message = \"Dime un chiste sobre gatos\" # El modelo identifica roles como system y user.\n",
        "Message Placeholder:mensajes = [     {\"role\": \"user\", \"content\": \"Hola\"},     {\"role\": \"user\", \"content\": \"¿Cómo estás?\"} ]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "m-8NEZl-dcnm"
      }
    }
  ]
}